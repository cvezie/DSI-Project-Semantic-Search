{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***DSI Project - Semantic Search***\n",
    "# Load Data\n",
    "\n",
    "## Goals:\n",
    "* Write functions to get sub-categories from categories and page text within each of those categories.\n",
    "* Connect to MongoDB.\n",
    "* Traverse through Wiki APIs to collect page text in a MongoDB server.\n",
    "\n",
    "\n",
    "## Output:\n",
    "* Cleaned data is inputted into MongoDB server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/dsi/assignments/p4\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-3.5.1-cp36-cp36m-manylinux1_x86_64.whl (365kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functions\n",
    "### `get_cats_and_pages` : Get the names of the children and pages in a Wikipedia API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cats_and_pages(category):\n",
    "    \"\"\"\n",
    "    Scrape the category page from the Wikipedia API.\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    category: str\n",
    "        The name of the category to be scraped.\n",
    "        \n",
    "    Returns: \n",
    "    ------\n",
    "    2 Lists\n",
    "        A list of subcategories.\n",
    "        A list of pages\n",
    "    \"\"\"\n",
    "    \n",
    "    my_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": \"Category:{}\".format(category),\n",
    "        \"cmlimit\": \"max\"   \n",
    "    }\n",
    "    r = requests.get(\"https://en.wikipedia.org/w/api.php?\", params=my_params)\n",
    "    \n",
    "    # Output info into a dataframe to easily separate the category into titles and pages. \n",
    "    cat = pd.DataFrame(r.json()['query']['categorymembers'])\n",
    "    \n",
    "    sub_categories = list(cat['title'][cat.title.str.contains('Category:')].str.replace('Category:', ''))\n",
    "    pages = list(cat['title'][~cat.title.str.contains('Category:')])\n",
    "    \n",
    "    \n",
    "    return sub_categories, pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_title` : Scrape the text from each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(title):\n",
    "    \"\"\"\n",
    "    Get the contents of a page from the Wikipedia API.\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    title: str\n",
    "        The name of the page to be scraped.\n",
    "        \n",
    "    Returns: \n",
    "    ------\n",
    "    List\n",
    "        String of the text on the page.\n",
    "    \"\"\"\n",
    "    \n",
    "    my_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    r = requests.get(\"https://en.wikipedia.org/w/api.php?\", params=my_params)\n",
    "    \n",
    "    pageid = list(r.json()['query']['pages'].values())[0]['pageid']\n",
    "    text = list(r.json()['query']['pages'].values())[0]['revisions'][0]['*']\n",
    "    \n",
    "    return pageid, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cleaner`: Cleans text before it gets to MongoDB\n",
    "* This function will run on each page after the `get_title` function retrieves text data and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    \"\"\"\n",
    "    Cleans text from wikipedia and removes characters and stop words\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    text: \n",
    "        The page that needs cleaning\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    Cleaned text\n",
    "    \"\"\"\n",
    "    text = re.sub('&#39;','',text).lower()\n",
    "    text = re.sub('<br />','',text)\n",
    "    text = re.sub('<.*>.*</.*>','', text)\n",
    "    text = re.sub('\\\\ufeff', '', text)\n",
    "    text = re.sub('[\\d]','',text)\n",
    "    text = re.sub('[^a-z ]','',text)\n",
    "    text = ' '.join(i.lemma_ for i in nlp(text)\n",
    "                    if i.orth_ not in STOP_WORDS)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dfs_traverse` : Recursive function to pull all data using previously written functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_dfs_traverse(category, db, max_depth=-1):\n",
    "    \"\"\"\n",
    "    Step 1: Splits a parent category into its pages and sub-categories.\n",
    "    Step 2: Gets the text data from all the pages in the category.\n",
    "    Step 3: Stores text data in database\n",
    "    Step 4: Re runs function through each sub-category until reaches max depth.  \n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    category: str\n",
    "        The name of the category to be scraped.\n",
    "    db: \n",
    "        The name of the database to store the information\n",
    "    max_depth: float\n",
    "        How deep to run the function. If no max_depth is selected, function will run until it reaches all leaves.\n",
    "        \n",
    "    Returns: \n",
    "    ------\n",
    "    Database contents\n",
    "        Cleaned data stored in connected database.\n",
    "    \"\"\"\n",
    "    \n",
    "    if max_depth == 0:\n",
    "        return\n",
    "    \n",
    "    print(category)\n",
    "    sub_categories, pages = get_cats_and_pages(category)\n",
    "\n",
    "    for page in pages:\n",
    "        pageid, text = get_title(page)\n",
    "        row = {\n",
    "        'pageid': pageid,\n",
    "        'text': cleaner(text),\n",
    "        'category': category\n",
    "        }\n",
    "        \n",
    "        db.insert_one(row)\n",
    "\n",
    "    for sub_cat in sub_categories:\n",
    "        wiki_dfs_traverse(sub_cat, db, max_depth=max_depth-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('34.215.225.199', 27016)\n",
    "db_ref = client.wiki_database\n",
    "wiki_ref = db_ref.wiki_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'local', 'wiki_bs_database', 'wiki_database']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Traverse\n",
    "* Call traverse function to retrieve Machine Learning data\n",
    "* `max_depth` is -1 to retrieve all data under the Machine Learning API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning\n",
      "Applied machine learning\n",
      "Artificial neural networks\n",
      "Deep learning\n",
      "Neural network software\n",
      "Bayesian networks\n",
      "Classification algorithms\n",
      "Artificial neural networks\n",
      "Deep learning\n",
      "Neural network software\n",
      "Decision trees\n",
      "Ensemble learning\n",
      "Cluster analysis\n",
      "Cluster analysis algorithms\n",
      "Clustering criteria\n",
      "Computational learning theory\n",
      "Artificial intelligence conferences\n",
      "Data mining and machine learning software\n",
      "Social network analysis software\n",
      "Datasets in machine learning\n",
      "Datasets in computer vision\n",
      "Dimension reduction\n",
      "Factor analysis\n",
      "Ensemble learning\n",
      "Evolutionary algorithms\n",
      "Gene expression programming\n",
      "Genetic algorithms\n",
      "Artificial immune systems\n",
      "Gene expression programming\n",
      "Genetic programming\n",
      "Nature-inspired metaheuristics\n",
      "Genetic programming\n",
      "Inductive logic programming\n",
      "Kernel methods for machine learning\n",
      "Support vector machines\n",
      "Latent variable models\n",
      "Factor analysis\n",
      "Structural equation models\n",
      "Learning in computer vision\n",
      "Log-linear models\n",
      "Loss functions\n",
      "Machine learning algorithms\n",
      "Genetic algorithms\n",
      "Artificial immune systems\n",
      "Gene expression programming\n",
      "Machine learning portal\n",
      "Machine learning task\n",
      "Markov models\n",
      "Hidden Markov models\n",
      "Markov networks\n",
      "Machine learning researchers\n",
      "Semisupervised learning\n",
      "Statistical natural language processing\n",
      "Language modeling\n",
      "Structured prediction\n",
      "Graphical models\n",
      "Bayesian networks\n",
      "Causal inference\n",
      "Markov networks\n",
      "Supervised learning\n",
      "Support vector machines\n",
      "Unsupervised learning\n"
     ]
    }
   ],
   "source": [
    "ml = wiki_dfs_traverse(\"Machine learning\", wiki_ref, max_depth=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Business Software Traverse\n",
    "* Call traverse function to retrieve Business Software data\n",
    "* `max_depth` is 2 to retrieve only the first two levels of data from Business Software so that the number of pages retrieved is roughly equal to the number of Machine Learning pages retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business software\n",
      "Administrative software\n",
      "Business simulation games\n",
      "Business software companies\n",
      "Business software for Linux\n",
      "Business software for MacOS\n",
      "Business software for Windows\n",
      "Collaborative software\n",
      "Dental practice management software\n",
      "Enterprise software\n",
      "ERP software\n",
      "Financial software\n",
      "Free business software\n",
      "Health software\n",
      "Healthcare software\n",
      "Human resource management software\n",
      "Java enterprise platform\n",
      "Manufacturing software\n",
      "Marketing software\n",
      "MES software\n",
      "Mobile business software\n",
      "Office software\n",
      "Portal software\n",
      "Project management software\n",
      "Publishing software\n",
      "Reporting software\n",
      "Risk management software\n",
      "Service-oriented architecture-related products\n",
      "Tax software\n",
      "Telecommunications Billing Systems\n",
      "Workflow technology\n",
      "Industry-specific XML-based standards\n",
      "Business software stubs\n"
     ]
    }
   ],
   "source": [
    "bs = wiki_dfs_traverse(\"Business software\", wiki_ref, max_depth=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
